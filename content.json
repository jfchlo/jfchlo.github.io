{"meta":{"title":"映 像","subtitle":null,"description":null,"author":"JF","url":"https://jfchl.cn"},"pages":[],"posts":[{"title":"Pagerank 实现文本摘要","slug":"pagerank实现文本摘要","date":"2019-03-21T14:39:57.000Z","updated":"2019-03-21T15:24:25.996Z","comments":true,"path":"2019/03/21/pagerank实现文本摘要/","link":"","permalink":"https://jfchl.cn/2019/03/21/pagerank实现文本摘要/","excerpt":"","text":"pagerank 实现文本自动摘要一 分句 使用正则将文档按照标点符号或其它符号进行分句，成为列表形式。 二 分词，去掉停用词 使用jieba分词将列表中的每个句子分词，并去掉停用词。这一步，还有词的向量化 可使用sklearn中的CountVectorizer函数一并实现。 词的向量化和tf-idf TFIDF 是个什么鬼 TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TFIDF实际上是：TF * IDF，TF词频(Term Frequency)，IDF逆向文件频率(Inverse Document Frequency)。 pagerank 这个pagerank 可以由networkx包实现 PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 根据网页之间相互的超链接计算的技术，而作为网页排名的要素之一，以Google公司创办人拉里·佩奇（Larry Page）之姓来命名 Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。 实现代码1234567891011121314151617181920212223242526272829303132333435363738394041424344import re# import numpy as npimport networkx as nximport jiebafrom sklearn.feature_extraction.text import TfidfTransformer,CountVectorizertxt=''with open('./1.txt',encoding='utf-8') as f: for line in iter(f): txt+=''.join(line)stop_word=['的','是','了','我','之','以','，','、','《','》','“','”']def get_sentences(text): #分句 s=re.split(r'[。,?,!,\\n\\n ,:]', text) sentences=[w for w in s if w!='' and len(w)&gt;0 ] return sentences#得分topn的句子作为摘要def textrank(sentences,n): sorted_sentences=[] #词向量化,使用jieba的cut方法 bow_matrix=CountVectorizer(tokenizer=jieba.cut,stop_words=stop_word).fit_transform(sentences) normalized=TfidfTransformer().fit_transform(bow_matrix) sim_graph=normalized*normalized.T # print(sim_graph) #将稀疏矩阵转化化为图（networkx） nx_graph=nx.from_scipy_sparse_matrix(sim_graph) scores=nx.pagerank(nx_graph) # print(scores) #字典排序 sorted_scores=sorted(scores.items(),key = lambda item: item[1], reverse=True) #print(sorted_scores) for index,score in sorted_scores: item=&#123; 'index':index, 'sentence':sentences[index], 'weight':score &#125; sorted_sentences.append(item) return sorted_sentences[:n] 12345t=get_sentences(txt)#print(textrank(t,5))for i in textrank(t,5): print(i) print('\\n') {‘index’: 16, ‘sentence’: ‘本研究使用总和生育率、递进生育率（控制了年龄、孩次后的总和生育率）、内在生育率（控制了年龄、孩次、生育间隔后的总和生育率）和队列生育率（一个出生队列的女性，到某一年龄为止的累计生育率或平均生育子女数）这四种生育率指标，来估计和对比分析中国近年来的生育水平与变化趋势’, ‘weight’: 0.020681738055241223} {‘index’: 58, ‘sentence’: ‘总之，类似于二孩递进总和生育率，二孩内在总和生育率的变化进一步表明近年来二孩总和生育率的大幅度上升是生育堆积效应的反映，但其对实际二孩生育水平的估计要高于二孩递进总和生育率’, ‘weight’: 0.018873167326076482} {‘index’: 55, ‘sentence’: ‘通过对比二孩总和生育率、二孩递进总和生育率和二孩内在总和生育率，可以看出因近10年来生育间隔的变化，二孩递进总和生育率对二孩生育水平存在一定程度低估，而二孩总和生育率在2012年前低估二孩生育水平，之后又高估二孩生育水平’, ‘weight’: 0.018784835700268225} {‘index’: 72, ‘sentence’: ‘综上，近年来一孩总和生育率的大幅度下降并不表明一孩生育水平的真实的有如此明显下降，而主要是反映了妇女婚育年龄推迟的进度效应，一孩总和生育率降到了0.6左右，而一孩内在总和生育率仍然高达0.9以上’, ‘weight’: 0.018205850700340118} {‘index’: 33, ‘sentence’: ‘在2016和2017年一孩生育水平较低的情况下总和生育率仍有较大幅度回升，原因就在于二孩总和生育率提高产生的对总和生育率的提升效应，这也表明了全面两孩政策效应非常显著’, ‘weight’: 0.018115375202097012} 摘要原文档地址: 中国家庭︱中国近10年的生育水平与趋势","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://jfchl.cn/categories/数据分析/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://jfchl.cn/tags/数据分析/"}]},{"title":"K-Means基于RFM模型的客户分类","slug":"基于RFM模型的客户分类","date":"2018-11-02T01:39:57.000Z","updated":"2018-11-02T02:23:21.994Z","comments":true,"path":"2018/11/02/基于RFM模型的客户分类/","link":"","permalink":"https://jfchl.cn/2018/11/02/基于RFM模型的客户分类/","excerpt":"","text":"看到过基于LRFMC模型的航空客户分类，试试对我司的客户进行分类。 如何通过RFM模型，为用户分群，实现精细化运营 RFM模型是一个被广泛使用的客户关系分析模型，主要以用户行为来区分客户，RFM分别是： R = Recency 最近一次消费 F = Frequency 消费频率 M = Monetary 消费金额 第一步：先挑出来近1个月的复购用户。 第二步：近1个月内复购用户的平均实付金额做纵轴。 第三步：近1个月内复购用户的购买次做横轴，生成表格。 第四步，你需要自己在这个表格上划红线。横着的红线,平均消费金额，竖着的红线，平均消费次数。 识别价值客户 R（消费时间间隔，最近消费时间距离观测点的时间间隔） M（当然是消费总额了） F（消费频次）， L（关系存续时长） C（平均折扣），这个C我没有 数据清洗 使用pandas 的dropna()函数去掉na值 drop()函数去掉件数为0的值 groupby 用法 drop_duplicates(column，keep=)去掉重复值，用到keep两个参数first和last first 保留第一次出现的值，得到第一次拣货的值 last 保留最后一次出现的值，得到最后一次拣货的值 pandas 的to_period 用法，去掉日期后面的时间值， Series.dt.days 获得天数 数据归一化 使用常见的标准差归一化数据 (x-x.mean(axis=0)/x.std(axis=0) K-Means 基本使用123#原始数据长相import pandas as pddata=pd.read_excel('e:/蒙捷物流/门店拣货数量.xls',index_column=None) WARNING *** file size (53730112) not 512 + multiple of sector size (512) 123#只保留有用字段data1=data[['生成时间','拣货门店','件数']]data1.info() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 54879 entries, 0 to 54878 Data columns (total 3 columns): 生成时间 54877 non-null datetime64[ns] 拣货门店 54877 non-null object 件数 54877 non-null object dtypes: datetime64[ns](1), object(2) memory usage: 1.3+ MB 1data1.head() 1data1.tail() 123456#处理NA和0值data1=data1.dropna()data1=data1.drop(data1[data1.件数=='0'].index)#设置索引# data1=data1.set_index('拣货门店')data1.head() 12345#修改件数字段为int64,用于统计数量import ref=lambda x:int(re.sub('\\D','',x))data1.件数=data1.件数.map(f)data1.info() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; Int64Index: 54257 entries, 6 to 54876 Data columns (total 3 columns): 生成时间 54257 non-null datetime64[ns] 拣货门店 54257 non-null object 件数 54257 non-null int64 dtypes: datetime64[ns](1), int64(1), object(1) memory usage: 1.7+ MB 123456789101112131415161718192021222324#各个门店总件数M=data1.groupby('拣货门店').sum()['件数']#各门店总计拣货次数Times=data1.groupby('拣货门店').count()['件数']#第一次拣货时间First_time=data1.drop_duplicates('拣货门店',keep='first')#最后一次拣货时间Last_time=data1.drop_duplicates('拣货门店',keep='last')#设置index,First_time=First_time.set_index('拣货门店')['生成时间']Last_time=Last_time.set_index('拣货门店')['生成时间']#拼接d=pd.concat([M,Times,First_time,Last_time],axis=1)d.columns=['M','F','First_time','Last_time']#设置观测点时间为2018-09-20日end_time=pd.to_datetime('2018-09-20')#客户存续时长，取天数d['L']=(d.Last_time-First_time).dt.days#删除L异常值d=d.drop(d[d['L']&lt;=0].index)#拣货间隔Rd['R']=(end_time-d.Last_time).dt.days#拣货频率d['ZF']=d['L']/d['F'] 1d1=data1.set_index('生成时间') 12d=d[['M','L','R','ZF']]d.head() 12#归一化d1=d.apply(lambda x: (x-x.mean(axis=0))/x.std(axis=0)) 1d1.head() 123from sklearn.cluster import KMeanskmode=KMeans(n_clusters=4,n_jobs=4,max_iter=1000)kmode.fit(d1) KMeans(algorithm=&apos;auto&apos;, copy_x=True, init=&apos;k-means++&apos;, max_iter=1000, n_clusters=4, n_init=10, n_jobs=4, precompute_distances=&apos;auto&apos;, random_state=None, tol=0.0001, verbose=0) 12#聚类中心kmode.cluster_centers_ array([[-0.20778844, -1.13408314, 1.30125341, -0.49642668], [-0.24994031, -0.22291813, 0.01485693, 1.47776371], [ 0.07186423, 0.61106574, -0.5932065 , -0.4025806 ], [ 9.00211981, 0.41519412, -0.69211313, -0.83961275]]) 1kmode.labels_ array([2, 2, 2, ..., 0, 2, 0]) 12c1=pd.Series(kmode.labels_)c1.value_counts() 可以看到 每一类 具体数量2 545 0 250 1 237 3 8 dtype: int64 1d['labels']=kmode.labels_ 1d[d['labels']==3]","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://jfchl.cn/categories/数据分析/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://jfchl.cn/tags/数据分析/"}]},{"title":"pandas 读取csv OSerro","slug":"坑2","date":"2018-10-15T20:29:03.000Z","updated":"2018-10-15T20:52:31.744Z","comments":true,"path":"2018/10/16/坑2/","link":"","permalink":"https://jfchl.cn/2018/10/16/坑2/","excerpt":"","text":"pandas读取csv文件如出现如下错误， 都是文件名或路径含有中文，改了就是了，也可加一句engine=’python’","categories":[{"name":"坑","slug":"坑","permalink":"https://jfchl.cn/categories/坑/"}],"tags":[{"name":"坑","slug":"坑","permalink":"https://jfchl.cn/tags/坑/"}]},{"title":"PEST分析方法（一）","slug":"PEST 分析模型","date":"2018-09-18T16:00:00.000Z","updated":"2018-09-19T01:58:39.062Z","comments":true,"path":"2018/09/19/PEST 分析模型/","link":"","permalink":"https://jfchl.cn/2018/09/19/PEST 分析模型/","excerpt":"","text":"PEST 分析模型 从事同城配送行业，感觉现在是王小二过年，心理慌啊，我还欠老马不少银子啊 看看这线条，跌宕起伏，犹如过山车叫人心惊肉跳。 耳闻PEST是卜卦大湿，据传卜过的人都说好。 一日登门请教！ 我：“大湿啊，你看看我这，未来可好？”大湿低头端详片刻，悠悠道：“你这是季节性情感障碍综合征。”我：“情感障碍？”大湿：“对，你看啊，节假日，特别是大的传统节日，你心情好点，而且越来越差，如今即便是传统节日，你的心情也大不如以前了。” 我：“大湿啊，你看看《武汉现代物流十三五规划纲要》，里头好多关于发展物流的热词，我还统计了一下，提到物流205个，还有这个配送。。。。。。”大湿抬头，轻蔑扫了我一眼，我立马收住激情。“一个物流规划文件，它不关注物流，关注什么，难道是某宝离婚，东哥性侵。” 大湿厉声道。 我默然！1234567891011121314151617181920212223import jiebaimport jieba.analyse from pyecharts import WordCloudfrom collections import Counterimport stringtxt=&apos;&apos;with open(&apos;e:/file/1.txt&apos;,mode=&apos;r&apos;,encoding=&apos;utf-8&apos;) as f: line=f.read() line=line.translate(string.punctuation)#删除标点及特殊字符 line=line.splitlines() txt=&apos;&apos;.join(line)keywords=jieba.cut(txt,cut_all=False)word=[]for key in keywords: if len(key)&gt;1: word.append(key)count=Counter(word)#print(count.most_common(50))name,value=WordCloud.cast(count)wordcloud = WordCloud(width=1000, height=400)wordcloud.add(&quot;&quot;, name, value,word_size_range=[20, 100])wordcloud 123456from pyecharts import Barname=count.most_common(30)m,n=Bar.cast(name)bar=Bar(&apos;热点词汇&apos;)bar.add(&apos;武汉市现代物流十三五规划&apos;,m,n)bar 1234567891011121314151617import pandas as pdfrom pyecharts import Lineimport refiles=&apos;e:/file/门店拣货数量.xls&apos;df=pd.read_excel(files,index_col=None)df1=df.loc[:,[&apos;生成时间&apos;,&apos;拣货门店&apos;,&apos;件数&apos;]]df1=df1.dropna()f=lambda x: int(re.sub(&apos;\\D&apos;,&apos;&apos;,x))df1[&apos;件数&apos;]=df1[&apos;件数&apos;].apply(f)p=df1.pivot_table(index=&apos;生成时间&apos;,values=&apos;件数&apos;,aggfunc=sum)t=p.resample(&apos;M&apos;).sum()t.values.Tline=Line()line.add(&quot;2017年至今运输数量&quot;, t.index, t.values.T[0], is_smooth=True, mark_line=[&quot;max&quot;, &quot;average&quot;])line PEST 为一种企业所处宏观环境分析模型，所谓PEST，即P是政治（Politics），E是经济（Economy），S是社会（Society），T是技术（Technology）. 这些是企业的外部环境，一般不受企业掌握，这些因素也被戏称为“pest（有害物）”PEST要求高级管理层具备相关的能力及素养。 政治环境 政府政策 法律环境 经济环境 社会经济结构 经济体制 宏观经济政策 当前经济状况 其他一般经济条件可以参考以下几点 利率 通货膨胀与人均就业 人均GDP的长远预期 社会环境 人口社会流动性 消费心理 生活方式变化 文化传统 价值观 技术环境","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://jfchl.cn/categories/数据分析/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://jfchl.cn/tags/数据分析/"}]},{"title":"excel string to int","slug":"坑","date":"2018-09-16T12:44:15.000Z","updated":"2018-09-19T01:38:37.977Z","comments":true,"path":"2018/09/16/坑/","link":"","permalink":"https://jfchl.cn/2018/09/16/坑/","excerpt":"","text":"读取excel数据string 转int问题 在导入excel数据进行计算时，有时需要将string转换int或者float因为python假设需要进行int转型的字符串仅仅包含数字，去除掉字符串中的非数字字符即可。","categories":[{"name":"坑","slug":"坑","permalink":"https://jfchl.cn/categories/坑/"}],"tags":[{"name":"坑","slug":"坑","permalink":"https://jfchl.cn/tags/坑/"}]},{"title":"JupyterToMarkdownTest","slug":"JupyterToMarkdownTest","date":"2018-09-09T22:20:51.000Z","updated":"2018-09-12T15:03:52.181Z","comments":true,"path":"2018/09/10/JupyterToMarkdownTest/","link":"","permalink":"https://jfchl.cn/2018/09/10/JupyterToMarkdownTest/","excerpt":"","text":"这是以前用jupyter，今日转换成md放到这里作为一个测试 销售描述统计与 matplotlib pandas 数据重采样 pandas 数据透视表 matplotlib 中文及符号显示 基本画图 以一月份份的销售为例： 123456import osimport pandas as pdimport matplotlib.pyplot as pltname=['单号','销售日期','商品编码','商品名称','车型','仓位','销售数量','销售单价','销售金额']os.chdir('e:')df1=pd.read_excel('./zyxiaoshou.xlsx',sheetname='Sheet5',header=0,names=name) 1df1.head() 这是一月份总体销售情况，来做个初步的统计描述,总销售金额，各个仓位的销售金额(包含了销售退单）看看pandas的透视表函授怎么实现上述功能 pd.pivot_table 12sel_table=df1.pivot_table(index='仓位',values=['销售金额'],aggfunc=sum,margins_name='销售总计',margins=True)sel_table 12sel_table=df1.pivot_table(index='商品名称',values=['销售数量','销售金额'],aggfunc=sum)sel_table.sort_values(by='销售金额',ascending=False)[0:10] 销售数据在一个工作薄中8个不同滴表中，看否能合并为一个 12345678910import osimport pandas as pdimport matplotlib.pyplot as pltname=['单号','销售日期','商品编码','商品名称','车型','仓位','销售数量','销售单价','销售金额']os.chdir('e:')df=[]for i in range(1,13): df.append(pd.read_excel('./zyxiaoshou.xlsx',sheetname='Sheet'+str(i),header=0,names=name))data=pd.concat(df)data.index=[data['销售日期']] 1data.tail() 1len(data) 33932 1from datetime import datetime 12sel_table=data.pivot_table(index='销售日期',columns='仓位',values='销售金额',aggfunc=sum,fill_value=0)sel_table.head() 12#每月各个仓库的销售汇总sel_table.resample('m').sum() 123456789101112t1=sel_table.resample('m').sum().ix[:,['福田库','王牌库','AA','AB','整车','轮胎库']]plt.rcParams['font.sans-serif']=['SimHei']plt.rcParams['axes.unicode_minus']=Falset1.plot(figsize=(10,3),rot=30)plt.legend()plt.grid(True)plt.xlabel('月份')plt.ylabel('销售金额')plt.title('2016年1-8月份分库销售统计')plt.show() 1234567t2=t1.resample('m').sum().sum(axis=1)t2.plot(figsize=(10,3),rot=30)plt.grid(True)plt.ylabel('销售金额(万元)')plt.xlabel('月份')plt.show() 123456789101112t1.sum(axis=1).plot(figsize=(10,3),rot=45,style='bo-')plt.grid(True)plt.title('总体销售')plt.ylabel('销售金额')plt.xlabel('销售月份')for i in range(len(t1)): plt.annotate(t1.sum(axis=1).values[i], xy=(t1.index[i],t1.sum(axis=1)[i]), xytext=(t1.index[i],t1.sum(axis=1)[i]) ) plt.show() 123import numpy as npa=pd.Series([1,2,3])a[1] 2 12t1.plot(subplots=True,figsize=(6,10),sharey=True,grid=True)plt.show() 12t1.plot(subplots=True,figsize=(14,6),layout=(2,3),sharey=True,sharex=False,grid=True)plt.show() 12t1.sum(axis=0).plot.pie(figsize=(6,6),autopct='%.2f')plt.show() 12t1.plot.area(figsize=(12,6))plt.show() 12345678910import osimport pandas as pdimport matplotlib.pyplot as pltname=['单号','销售日期','商品编码','商品名称','车型','仓位','销售数量','销售单价','销售金额']os.chdir('e:')df=[]for i in range(1,12): df.append(pd.read_excel('./zyxiaoshou.xlsx',sheetname='Sheet'+str(i),header=0,names=name))data=pd.concat(df)data.to_excel('./zyqp.xls')","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://jfchl.cn/categories/数据分析/"}],"tags":[{"name":"jupyter markdown","slug":"jupyter-markdown","permalink":"https://jfchl.cn/tags/jupyter-markdown/"}]},{"title":"青海湖","slug":"qinghaihu","date":"2018-09-08T14:04:51.000Z","updated":"2018-09-08T14:22:36.745Z","comments":true,"path":"2018/09/08/qinghaihu/","link":"","permalink":"https://jfchl.cn/2018/09/08/qinghaihu/","excerpt":"","text":"","categories":[{"name":"PHOTOS","slug":"PHOTOS","permalink":"https://jfchl.cn/categories/PHOTOS/"}],"tags":[{"name":"photos","slug":"photos","permalink":"https://jfchl.cn/tags/photos/"}]},{"title":"justifiedGallery-test","slug":"facybox-test","date":"2018-09-05T07:04:51.000Z","updated":"2018-09-05T12:40:24.958Z","comments":true,"path":"2018/09/05/facybox-test/","link":"","permalink":"https://jfchl.cn/2018/09/05/facybox-test/","excerpt":"","text":"本想出发之前完成。。。。。。","categories":[{"name":"PHOTOS","slug":"PHOTOS","permalink":"https://jfchl.cn/categories/PHOTOS/"}],"tags":[{"name":"photos","slug":"photos","permalink":"https://jfchl.cn/tags/photos/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-25T14:17:16.784Z","updated":"2018-07-30T05:52:19.237Z","comments":true,"path":"2018/07/25/hello-world/","link":"","permalink":"https://jfchl.cn/2018/07/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}